DATASET: LatinBHO
DATA_LOADER:
  NUM_THREADS: 12
MODEL:
  STYLE_ENCODER_LAYERS: 3
  NUM_IMGS: 15
  IN_CHANNELS: 4
  OUT_CHANNELS: 4
  NUM_RES_BLOCKS: 1
  NUM_HEADS: 4
  EMB_DIM: 512
SOLVER:
  BASE_LR: 0.00005  # Increased - training is stable now
  EPOCHS: 2000
  WARMUP_ITERS: 5000  # Reduced warmup - model learns quickly
  TYPE: AdamW
  GRAD_L2_CLIP: 2.0  # Relaxed - was over-clipping good gradients
  DIVERSITY_WEIGHT: 0.0  # Disabled - model is stable without it
  DIVERSITY_MIN_VARIANCE: 0.1
  DIVERSITY_START_ITER: 5000
TRAIN:
  TYPE: train
  IMS_PER_BATCH: 10  # Main training batch size
  IMG_H: 64
  IMG_W: 64
  IMAGE_PATH: data/LatinBHO/images
  STYLE_PATH: data/LatinBHO/style_images
  LABEL_PATH: data/LatinBHO/train.txt
  SNAPSHOT_ITERS: 500  # Save checkpoint every 500 iterations with test image
  SNAPSHOT_BEGIN: 0
  SEED: 1001
# PyLaia readability supervisor settings
PYLAIA:
  ENABLED: true  # Enable PyLaia CTC loss for readability supervision
  # Use weights-only checkpoint (extracted without Lightning dependencies)
  CHECKPOINT: ../bootstrap_training_data/pylaia_models/model_v22/weights_only.pth
  SYMS_PATH: ../bootstrap_training_data/pylaia_models/model_v22/syms.txt
  INPUT_HEIGHT: 128  # PyLaia expects 128px height
  WEIGHT: 0.02  # Increased - stronger readability signal
  START_ITER: 2000  # Start earlier - model learns basic shapes quickly
  APPLY_EVERY: 1  # Apply CTC loss every batch (was 32, reduced batch_size allows this)
  BATCH_SIZE: 2  # Number of samples to use for CTC loss
TEST:
  TYPE: test
  IMS_PER_BATCH: 8
  IMG_H: 64
  IMAGE_PATH: data/LatinBHO/images
  STYLE_PATH: data/LatinBHO/style_images
  LABEL_PATH: data/LatinBHO/test.txt

